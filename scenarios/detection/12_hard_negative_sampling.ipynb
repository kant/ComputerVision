{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hard Negative Sampling for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You built an image classification model, evaluated it on a validation set and got a decent accuracy. Now you deploy the model for the real-world scenario. And soon, you may find that the model performs worse than expected.\n",
    "\n",
    "This is quite common scenario (and inevitable) when we build a machine learning model because we cannot collect all the possible samples. Your model is supposed to learn the features that describe the target classes the best, but in reality, it learns the best features to classify your dataset. For example, if we have photos of *butterfly* on a flower, the model may learn flower shapes to classify *butterfly*.\n",
    "\n",
    "<img src=\"./media/hard_neg_ex1.jpg\" width=\"300\"> | <img src=\"./media/hard_neg_ex2.jpg\" width=\"300\"> \n",
    "---|---\n",
    "Did our model learn a butterfly? | or yellow flowers?\n",
    "\n",
    "Hard negative sampling (or hard negative mining) is a useful technique to address this pitfall. It is a way to explicitly create examples for your training set from falsely classified samples. The technique is widely used when you cannot add all the negative samples since (i) training time would get too slow because of too many training samples; and (ii) many of the negative images are trivial for the model and hence the model would not learn anything. Therefore, we try to identify the images which make a difference when added to the training set.\n",
    "\n",
    "In this notebook, we train our model on a training set as usual, test the model on un-seen negative examples and see if the model classifies them correctly. If not, we introduce those samples into the training set and re-train the model on them.\n",
    "\n",
    "# Overview\n",
    "\n",
    "Our goal is to train a classifier which can recognize *fridge obejcts* (`watter_bottle`, `carton`, `can`, and `milk_bottle`), similar to [01_train notebook](./01_training_introduction.ipynb). However, the input image might not even contain any of these objects in the real use-case. Therefore, we also introduce `negative` class.\n",
    "\n",
    "<img src=\"./media/hard_neg.jpg\" width=\"600\"/>\n",
    "\n",
    "The overall training process is as follows: \n",
    "* First, prepare training set <i>T</i> and negative-sample set <i>U</i>. <i>T</i> may include initial negative samples\n",
    "* Next, load a pre-trained ImageNet model\n",
    "* And then, mine hard negative samples by following steps as shown in the figure:\n",
    "    1. Train the model on <i>T</i>\n",
    "    2. Score the model on <i>U</i>\n",
    "    3. Identify hard images the model mis-classified, annotate them and add to <i>T</i> so that the model can learn the patterns it confused before.\n",
    "* Finally, repeat these steps until we get a good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TorchVision: 0.4.0\n",
      "Torch is using GPU: Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Iterator\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from random import randrange\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import scrapbook as sb\n",
    "\n",
    "from utils_cv.common.data import unzip_url, data_path\n",
    "from utils_cv.detection.data import Urls\n",
    "from utils_cv.classification.data import Urls as UrlsIC\n",
    "from utils_cv.detection.dataset import DetectionDataset\n",
    "from utils_cv.detection.plot import (\n",
    "    display_bboxes,\n",
    "    plot_grid,\n",
    "    plot_boxes,\n",
    "    plot_pr_curves,\n",
    "    PlotSettings,\n",
    "    plot_detection_vs_ground_truth,\n",
    ")\n",
    "from utils_cv.detection.model import DetectionLearner, _get_det_bboxes, get_pretrained_fasterrcnn\n",
    "from utils_cv.common.gpu import which_processor, is_windows\n",
    "\n",
    "# Change matplotlib backend so that plots are shown for windows\n",
    "if is_windows():\n",
    "    plt.switch_backend('TkAgg')\n",
    "\n",
    "print(f\"TorchVision: {torchvision.__version__}\")\n",
    "which_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure edits to libraries are loaded and plotting is shown in the notebook.\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch device: cuda\n"
     ]
    }
   ],
   "source": [
    "#DATA_PATH = unzip_url(Urls.fridge_objects_path, exist_ok=True)\n",
    "DATA_PATH = \"C:/Users/pabuehle/Desktop/ComputerVision/data/odRefrigerator\"\n",
    "NEG_DATA_PATH = unzip_url(UrlsIC.fridge_objects_negatives_path, exist_ok=True)\n",
    "\n",
    "# Number of negative samples to add for each iteration of negative mining\n",
    "NEGATIVE_NUM = 10\n",
    "\n",
    "# Using fast_inference parameters from 03_training_accuracy_vs_speed notebook.\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.005\n",
    "BATCH_SIZE = 2 \n",
    "\n",
    "IM_SIZE = 600\n",
    "rpn_pre_nms_top_n_train = 2000\n",
    "rpn_pre_nms_top_n_test = 1000 \n",
    "rpn_post_nms_top_n_train = 2000 \n",
    "rpn_post_nms_top_n_test = 1000\n",
    "\n",
    "# Temporary folder to store datasets for hard-negative mining\n",
    "NEGATIVE_MINING_DATA_DIR = TemporaryDirectory().name\n",
    "\n",
    "# Train on the GPU or on the CPU, if a GPU is not available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using torch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare our dataset in the following way:\n",
    "* The initial training set `T` to include *fridge objects* as well as some initial *negative samples*<sup>+</sup>.\n",
    "* Negative image set `U`.\n",
    "* Validation set `V` to have both *fridge objects* and *negative samples*. We evaluate our model on this set.\n",
    "\n",
    "<sub>+ We added `NEGATIVE_NUM` of negative samples to our initial training set. In a real use-case, you may want to include 100 or more images of negative samples.</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../..\\utils_cv\\detection\\dataset.py:62: FutureWarning: The behavior of this method will change in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if root.find(\"path\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 23 | Training DataLoader: <torch.utils.data.dataloader.DataLoader object at 0x0000018A8F09DB00> \n",
      "Testing dataset: 7 | Testing DataLoader: <torch.utils.data.dataloader.DataLoader object at 0x0000018A8F0B5198>\n",
      "Negative dataset split into 52 candidates for hard negative mining, and 12 test images.\n"
     ]
    }
   ],
   "source": [
    "#ori_datapath = Path(DATA_PATH)\n",
    "# # We split positive samples into 80% training and 20% validation\n",
    "# data_imlist = (\n",
    "#     ImageList.from_folder(ori_datapath)\n",
    "#     .split_by_rand_pct(valid_pct=0.2, seed=10)\n",
    "#     .label_from_folder()\n",
    "# )\n",
    "\n",
    "# We use 80% of negative images for hard-negative mining (set U) while 20% for validation\n",
    "#neg_datapath = Path(NEG_DATA_PATH)\n",
    "# neg_data = (\n",
    "#     ImageList.from_folder(neg_datapath)\n",
    "#     .split_by_rand_pct(valid_pct=0.2, seed=10)\n",
    "#     .label_const()  # We don't use labels for negative data\n",
    "#     .transform(size=IMAGE_SIZE)\n",
    "#     .databunch(bs=BATCH_SIZE, num_workers = db_num_workers())\n",
    "#     .normalize(imagenet_stats)\n",
    "# )\n",
    "# # Do not shuffle U when we predict\n",
    "# neg_data.train_dl = neg_data.train_dl.new(shuffle=False)\n",
    "\n",
    "# We split positive samples into 75% training and 25% validation\n",
    "data = DetectionDataset(DATA_PATH, train_pct=0.75)\n",
    "print(\n",
    "    f\"Training dataset: {len(data.train_ds)} | Training DataLoader: {data.train_dl} \\nTesting dataset: {len(data.test_ds)} | Testing DataLoader: {data.test_dl}\"\n",
    ")\n",
    "\n",
    "# We use most of negative images for hard-negative mining (set U) and remaining to test how often \n",
    "# our model incorrectly fires on these negative images.\n",
    "neg_data = DetectionDataset(NEG_DATA_PATH, train_pct=0.80, batch_size=BATCH_SIZE, \n",
    "                            im_dir = \"\", require_annotation_files = False)\n",
    "print(\n",
    "    f\"Negative dataset split into {len(neg_data.train_ds)} candidates for hard negative mining, and {len(neg_data.test_ds)} test images.\"\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MAYBE NO NEED TO DO THAT HERE. COULD JUST ADD FIRST TIME THE MOST RELEVANT ONES.\n",
    "\n",
    "# Add random NEGATIVE_NUM negatives to the training data\n",
    "neg_im_indices = np.random.randint(len(neg_data.train_ds.dataset.im_paths)-1, size=NEGATIVE_NUM)\n",
    "for idx in neg_im_indices:\n",
    "    data.im_paths.append(neg_data.train_ds.dataset.im_paths[idx])\n",
    "    data.anno_bboxes.append(neg_data.train_ds.dataset.anno_bboxes[idx])\n",
    "    data.train_ds.indices.append(len(data.im_paths)-1)\n",
    "\n",
    "data.init_data_loaders()\n",
    "print(\n",
    "    f\"Training dataset: {len(data.train_ds)} | Training DataLoader: {data.train_dl} \\nTesting dataset: {len(data.test_ds)} | Testing DataLoader: {data.test_dl}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = Path(NEGATIVE_MINING_DATA_DIR)/'data'\n",
    "\n",
    "# # Training set T\n",
    "# copy_files(data_imlist.train.items, datapath/'train', infer_subdir=True)\n",
    "# # We include first NEGATIVE_NUM negative images in U (neg_data.train_ds) to our initial training set T\n",
    "# copy_files(neg_data.train_ds.items[:NEGATIVE_NUM], datapath/'train'/'negative')\n",
    "\n",
    "# # Validation set V\n",
    "# copy_files(data_imlist.valid.items, datapath/'valid', infer_subdir=True)\n",
    "# copy_files(neg_data.valid_ds.items, datapath/'valid'/'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_random_seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = (\n",
    "#     ImageList.from_folder(datapath)\n",
    "#     .split_by_folder()\n",
    "#     .label_from_folder()\n",
    "#     .transform(size=IMAGE_SIZE)\n",
    "#     .databunch(bs=BATCH_SIZE, num_workers = db_num_workers())\n",
    "#     .normalize(imagenet_stats)\n",
    "# )\n",
    "# data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare a model\n",
    "\n",
    "We use *fast inference* setup we demonstrated from [02_training_accuracy_vs_speed notebook](./02_training_accuracy_vs_speed.ipynb). The model is Resnet18 and pre-trained on [ImageNet](http://www.image-net.org/). Regarding the details about training concept, please see [01_training notebook](./01_training_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: <class 'torchvision.models.detection.faster_rcnn.FasterRCNN'>\n"
     ]
    }
   ],
   "source": [
    "model = get_pretrained_fasterrcnn(\n",
    "    num_classes=len(data.labels) + 1, min_size=IM_SIZE, max_size=IM_SIZE\n",
    ")\n",
    "detector = DetectionLearner(data, model=model)\n",
    "print(f\"Model: {type(detector.model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We record train and valid accuracies for later analysis\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "interpretations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the model on *T*\n",
    "\n",
    "<a id='train'></a>\n",
    "\n",
    "From this section to the end, we do training and negative mining. As described in the Overview section, You may need to do repeat the negative mining steps several times to achieve good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran 0 time(s)\n"
     ]
    }
   ],
   "source": [
    "# Show the number of repetitions you went through the negative mining\n",
    "print(f\"Ran {len(interpretations)} time(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/17]  eta: 0:04:50  lr: 0.000317  loss: 3.8255 (3.8255)  loss_classifier: 1.6257 (1.6257)  loss_box_reg: 0.2987 (0.2987)  loss_objectness: 1.5744 (1.5744)  loss_rpn_box_reg: 0.3267 (0.3267)  time: 17.1056  data: 0.7979  max mem: 883\n",
      "Epoch: [0]  [16/17]  eta: 0:00:01  lr: 0.005000  loss: 1.2414 (1.6329)  loss_classifier: 0.6446 (0.7768)  loss_box_reg: 0.2987 (0.3218)  loss_objectness: 0.0804 (0.3523)  loss_rpn_box_reg: 0.1332 (0.1821)  time: 1.2107  data: 0.1327  max mem: 1329\n",
      "Epoch: [0] Total time: 0:00:20 (1.2116 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:00:01  model_time: 0.1250 (0.1250)  evaluator_time: 0.0156 (0.0156)  time: 0.2656  data: 0.1250  max mem: 1329\n",
      "Test:  [3/4]  eta: 0:00:00  model_time: 0.0312 (0.0586)  evaluator_time: 0.0156 (0.0117)  time: 0.1797  data: 0.1055  max mem: 1329\n",
      "Test: Total time: 0:00:00 (0.1836 s / it)\n",
      "Averaged stats: model_time: 0.0312 (0.0586)  evaluator_time: 0.0156 (0.0117)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.03s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.265\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.573\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.150\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.451\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.253\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.236\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.378\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.567\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.422\n",
      "Epoch: [1]  [ 0/17]  eta: 0:00:04  lr: 0.005000  loss: 1.1738 (1.1738)  loss_classifier: 0.6374 (0.6374)  loss_box_reg: 0.4971 (0.4971)  loss_objectness: 0.0105 (0.0105)  loss_rpn_box_reg: 0.0288 (0.0288)  time: 0.2737  data: 0.1488  max mem: 1329\n",
      "Epoch: [1]  [16/17]  eta: 0:00:00  lr: 0.005000  loss: 0.7694 (0.7742)  loss_classifier: 0.2706 (0.3113)  loss_box_reg: 0.2272 (0.2522)  loss_objectness: 0.0227 (0.0439)  loss_rpn_box_reg: 0.1471 (0.1668)  time: 0.2108  data: 0.0996  max mem: 1329\n",
      "Epoch: [1] Total time: 0:00:03 (0.2112 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:00:00  model_time: 0.0469 (0.0469)  evaluator_time: 0.0156 (0.0156)  time: 0.1886  data: 0.1261  max mem: 1329\n",
      "Test:  [3/4]  eta: 0:00:00  model_time: 0.0312 (0.0352)  evaluator_time: 0.0156 (0.0156)  time: 0.1604  data: 0.1096  max mem: 1329\n",
      "Test: Total time: 0:00:00 (0.1604 s / it)\n",
      "Averaged stats: model_time: 0.0312 (0.0352)  evaluator_time: 0.0156 (0.0156)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.347\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.686\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.290\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.316\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.380\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.319\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.473\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.517\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.528\n",
      "Epoch: [2]  [ 0/17]  eta: 0:00:03  lr: 0.005000  loss: 0.8312 (0.8312)  loss_classifier: 0.4409 (0.4409)  loss_box_reg: 0.3544 (0.3544)  loss_objectness: 0.0133 (0.0133)  loss_rpn_box_reg: 0.0225 (0.0225)  time: 0.2344  data: 0.1250  max mem: 1329\n",
      "Epoch: [2]  [16/17]  eta: 0:00:00  lr: 0.005000  loss: 0.4895 (0.4794)  loss_classifier: 0.1762 (0.1858)  loss_box_reg: 0.1423 (0.1432)  loss_objectness: 0.0133 (0.0164)  loss_rpn_box_reg: 0.1117 (0.1341)  time: 0.2106  data: 0.1006  max mem: 1329\n",
      "Epoch: [2] Total time: 0:00:03 (0.2106 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:00:00  model_time: 0.0469 (0.0469)  evaluator_time: 0.0156 (0.0156)  time: 0.1875  data: 0.1250  max mem: 1329\n",
      "Test:  [3/4]  eta: 0:00:00  model_time: 0.0469 (0.0430)  evaluator_time: 0.0156 (0.0117)  time: 0.1526  data: 0.0979  max mem: 1329\n",
      "Test: Total time: 0:00:00 (0.1565 s / it)\n",
      "Averaged stats: model_time: 0.0469 (0.0430)  evaluator_time: 0.0156 (0.0117)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.466\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.749\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.548\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.632\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.490\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.423\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.599\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.599\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.721\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.653\n",
      "Epoch: [3]  [ 0/17]  eta: 0:00:04  lr: 0.005000  loss: 0.2614 (0.2614)  loss_classifier: 0.0933 (0.0933)  loss_box_reg: 0.0741 (0.0741)  loss_objectness: 0.0164 (0.0164)  loss_rpn_box_reg: 0.0776 (0.0776)  time: 0.2500  data: 0.1250  max mem: 1329\n",
      "Epoch: [3]  [16/17]  eta: 0:00:00  lr: 0.005000  loss: 0.2650 (0.2560)  loss_classifier: 0.1048 (0.1025)  loss_box_reg: 0.0707 (0.0685)  loss_objectness: 0.0097 (0.0099)  loss_rpn_box_reg: 0.0767 (0.0751)  time: 0.1898  data: 0.0835  max mem: 1329\n",
      "Epoch: [3] Total time: 0:00:03 (0.1898 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:00:00  model_time: 0.0312 (0.0312)  evaluator_time: 0.0156 (0.0156)  time: 0.1875  data: 0.1250  max mem: 1329\n",
      "Test:  [3/4]  eta: 0:00:00  model_time: 0.0312 (0.0352)  evaluator_time: 0.0000 (0.0078)  time: 0.1523  data: 0.1016  max mem: 1329\n",
      "Test: Total time: 0:00:00 (0.1523 s / it)\n",
      "Averaged stats: model_time: 0.0312 (0.0352)  evaluator_time: 0.0000 (0.0078)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.571\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.812\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.726\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.626\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.647\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.552\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.645\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.645\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.754\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4]  [ 0/17]  eta: 0:00:02  lr: 0.005000  loss: 0.1824 (0.1824)  loss_classifier: 0.0630 (0.0630)  loss_box_reg: 0.0313 (0.0313)  loss_objectness: 0.0051 (0.0051)  loss_rpn_box_reg: 0.0830 (0.0830)  time: 0.1562  data: 0.0625  max mem: 1329\n",
      "Epoch: [4]  [16/17]  eta: 0:00:00  lr: 0.005000  loss: 0.2012 (0.1977)  loss_classifier: 0.0630 (0.0681)  loss_box_reg: 0.0379 (0.0401)  loss_objectness: 0.0045 (0.0051)  loss_rpn_box_reg: 0.0795 (0.0844)  time: 0.1835  data: 0.0776  max mem: 1329\n",
      "Epoch: [4] Total time: 0:00:03 (0.1847 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:00:00  model_time: 0.0469 (0.0469)  evaluator_time: 0.0156 (0.0156)  time: 0.1875  data: 0.1250  max mem: 1329\n",
      "Test:  [3/4]  eta: 0:00:00  model_time: 0.0313 (0.0393)  evaluator_time: 0.0156 (0.0117)  time: 0.1607  data: 0.1057  max mem: 1329\n",
      "Test: Total time: 0:00:00 (0.1607 s / it)\n",
      "Averaged stats: model_time: 0.0313 (0.0393)  evaluator_time: 0.0156 (0.0117)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.580\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.869\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.816\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.688\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.622\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.559\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.612\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.612\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.692\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.681\n",
      "Epoch: [5]  [ 0/17]  eta: 0:00:02  lr: 0.005000  loss: 0.2102 (0.2102)  loss_classifier: 0.0463 (0.0463)  loss_box_reg: 0.0304 (0.0304)  loss_objectness: 0.0035 (0.0035)  loss_rpn_box_reg: 0.1299 (0.1299)  time: 0.1719  data: 0.0625  max mem: 1329\n",
      "Epoch: [5]  [16/17]  eta: 0:00:00  lr: 0.005000  loss: 0.1629 (0.1554)  loss_classifier: 0.0497 (0.0508)  loss_box_reg: 0.0355 (0.0320)  loss_objectness: 0.0035 (0.0049)  loss_rpn_box_reg: 0.0729 (0.0677)  time: 0.1916  data: 0.0831  max mem: 1330\n",
      "Epoch: [5] Total time: 0:00:03 (0.1916 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:00:00  model_time: 0.0469 (0.0469)  evaluator_time: 0.0000 (0.0000)  time: 0.1719  data: 0.1250  max mem: 1330\n",
      "Test:  [3/4]  eta: 0:00:00  model_time: 0.0312 (0.0391)  evaluator_time: 0.0000 (0.0078)  time: 0.1484  data: 0.0977  max mem: 1330\n",
      "Test: Total time: 0:00:00 (0.1523 s / it)\n",
      "Averaged stats: model_time: 0.0312 (0.0391)  evaluator_time: 0.0000 (0.0078)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.680\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.881\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.862\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.740\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.751\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.650\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.698\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.698\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.742\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.778\n",
      "Epoch: [6]  [ 0/17]  eta: 0:00:03  lr: 0.005000  loss: 0.1252 (0.1252)  loss_classifier: 0.0335 (0.0335)  loss_box_reg: 0.0247 (0.0247)  loss_objectness: 0.0021 (0.0021)  loss_rpn_box_reg: 0.0648 (0.0648)  time: 0.1875  data: 0.0625  max mem: 1330\n",
      "Epoch: [6]  [16/17]  eta: 0:00:00  lr: 0.005000  loss: 0.1276 (0.1386)  loss_classifier: 0.0335 (0.0338)  loss_box_reg: 0.0275 (0.0289)  loss_objectness: 0.0037 (0.0050)  loss_rpn_box_reg: 0.0650 (0.0709)  time: 0.1849  data: 0.0780  max mem: 1330\n",
      "Epoch: [6] Total time: 0:00:03 (0.1849 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:00:00  model_time: 0.0469 (0.0469)  evaluator_time: 0.0000 (0.0000)  time: 0.1717  data: 0.1248  max mem: 1330\n",
      "Test:  [3/4]  eta: 0:00:00  model_time: 0.0469 (0.0469)  evaluator_time: 0.0000 (0.0000)  time: 0.1526  data: 0.1057  max mem: 1330\n",
      "Test: Total time: 0:00:00 (0.1526 s / it)\n",
      "Averaged stats: model_time: 0.0469 (0.0469)  evaluator_time: 0.0000 (0.0000)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.659\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.886\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.831\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.704\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.747\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.634\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.680\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.680\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.708\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.769\n",
      "Epoch: [7]  [ 0/17]  eta: 0:00:02  lr: 0.000500  loss: 0.1789 (0.1789)  loss_classifier: 0.0427 (0.0427)  loss_box_reg: 0.0367 (0.0367)  loss_objectness: 0.0046 (0.0046)  loss_rpn_box_reg: 0.0949 (0.0949)  time: 0.1562  data: 0.0625  max mem: 1330\n",
      "Epoch: [7]  [16/17]  eta: 0:00:00  lr: 0.000500  loss: 0.1154 (0.1152)  loss_classifier: 0.0261 (0.0311)  loss_box_reg: 0.0227 (0.0226)  loss_objectness: 0.0042 (0.0044)  loss_rpn_box_reg: 0.0584 (0.0570)  time: 0.1835  data: 0.0774  max mem: 1330\n",
      "Epoch: [7] Total time: 0:00:03 (0.1835 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:00:00  model_time: 0.0469 (0.0469)  evaluator_time: 0.0000 (0.0000)  time: 0.1717  data: 0.1092  max mem: 1330\n",
      "Test:  [3/4]  eta: 0:00:00  model_time: 0.0312 (0.0312)  evaluator_time: 0.0000 (0.0078)  time: 0.1523  data: 0.1054  max mem: 1330\n",
      "Test: Total time: 0:00:00 (0.1523 s / it)\n",
      "Averaged stats: model_time: 0.0312 (0.0312)  evaluator_time: 0.0000 (0.0078)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.693\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.886\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.849\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.765\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.785\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.664\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.711\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.711\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.767\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8]  [ 0/17]  eta: 0:00:03  lr: 0.000500  loss: 0.0947 (0.0947)  loss_classifier: 0.0253 (0.0253)  loss_box_reg: 0.0229 (0.0229)  loss_objectness: 0.0092 (0.0092)  loss_rpn_box_reg: 0.0373 (0.0373)  time: 0.2031  data: 0.0937  max mem: 1330\n",
      "Epoch: [8]  [16/17]  eta: 0:00:00  lr: 0.000500  loss: 0.0914 (0.0874)  loss_classifier: 0.0244 (0.0263)  loss_box_reg: 0.0166 (0.0168)  loss_objectness: 0.0049 (0.0049)  loss_rpn_box_reg: 0.0338 (0.0394)  time: 0.1984  data: 0.0958  max mem: 1330\n",
      "Epoch: [8] Total time: 0:00:03 (0.1993 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:00:00  model_time: 0.0499 (0.0499)  evaluator_time: 0.0050 (0.0050)  time: 0.1798  data: 0.1249  max mem: 1330\n",
      "Test:  [3/4]  eta: 0:00:00  model_time: 0.0312 (0.0398)  evaluator_time: 0.0000 (0.0052)  time: 0.1481  data: 0.0992  max mem: 1330\n",
      "Test: Total time: 0:00:00 (0.1486 s / it)\n",
      "Averaged stats: model_time: 0.0312 (0.0398)  evaluator_time: 0.0000 (0.0052)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.697\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.886\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.849\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.804\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.774\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.670\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.720\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.720\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.804\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.807\n",
      "Epoch: [9]  [ 0/17]  eta: 0:00:02  lr: 0.000500  loss: 0.0659 (0.0659)  loss_classifier: 0.0182 (0.0182)  loss_box_reg: 0.0130 (0.0130)  loss_objectness: 0.0055 (0.0055)  loss_rpn_box_reg: 0.0292 (0.0292)  time: 0.1719  data: 0.0625  max mem: 1330\n",
      "Epoch: [9]  [16/17]  eta: 0:00:00  lr: 0.000500  loss: 0.0777 (0.0819)  loss_classifier: 0.0200 (0.0236)  loss_box_reg: 0.0130 (0.0151)  loss_objectness: 0.0031 (0.0038)  loss_rpn_box_reg: 0.0292 (0.0394)  time: 0.1842  data: 0.0766  max mem: 1330\n",
      "Epoch: [9] Total time: 0:00:03 (0.1842 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/4]  eta: 0:00:00  model_time: 0.0469 (0.0469)  evaluator_time: 0.0000 (0.0000)  time: 0.1719  data: 0.1250  max mem: 1330\n",
      "Test:  [3/4]  eta: 0:00:00  model_time: 0.0312 (0.0352)  evaluator_time: 0.0000 (0.0078)  time: 0.1523  data: 0.1055  max mem: 1330\n",
      "Test: Total time: 0:00:00 (0.1523 s / it)\n",
      "Averaged stats: model_time: 0.0312 (0.0352)  evaluator_time: 0.0000 (0.0078)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.05s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.707\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.886\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.849\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.791\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.800\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.674\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.723\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.723\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.796\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.817\n"
     ]
    }
   ],
   "source": [
    "detector.fit(EPOCHS, lr=LEARNING_RATE, print_freq=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows confusion matrix for the validation set. If you are repeating the negative mining steps, you will see all the confusion matrices from the repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.706998271255697"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get accuracy on test set at IOU=0.5:0.95\n",
    "acc = float(detector.ap[-1])\n",
    "valid_acc.append(acc)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpretations.append(ClassificationInterpretation.from_learner(learn))\n",
    "#for i, interp in enumerate(interpretations):\n",
    "#    interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store train and valid accuracy\n",
    "#train_acc.extend(np.array(learn.train_metrics_recorder.train_metrics)[:, 0])\n",
    "#valid_acc.extend(np.array(learn.train_metrics_recorder.valid_metrics)[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils_cv.common.plot import line_graph\n",
    "line_graph(\n",
    "    values=(train_acc), #, valid_acc),\n",
    "    labels=(\"Train\"), #, \"Valid\"),\n",
    "    x_guides=[i*EPOCHS for i in range(1, len(valid_acc)//EPOCHS + 1)],\n",
    "    x_name=\"Epoch\",\n",
    "    y_name=\"Accuracy\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the model performs well enough, we can stop the training / negative sampling here.**\n",
    "\n",
    "If not, let's do hard negative sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Score the model on *U* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = detector.predict_dl(neg_data.train_dl, threshold=0)\n",
    "#pred_outs = np.array(get_preds(learn, neg_data.train_dl)[0].tolist())\n",
    "#print(f\"Prediction results:\\n{pred_outs[:10]}\\n...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hard negative mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.237814798951149, 6),\n",
       " (0.8555631041526794, 10),\n",
       " (0.893003523349762, 12),\n",
       " (0.5839400887489319, 13),\n",
       " (0.4500601291656494, 14),\n",
       " (0.69319748878479, 17),\n",
       " (0.6458784937858582, 18),\n",
       " (0.2566404342651367, 22),\n",
       " (0.4924968481063843, 26),\n",
       " (0.7386099100112915, 31),\n",
       " (0.8763371706008911, 39),\n",
       " (0.9042614102363586, 41),\n",
       " (0.15307456254959106, 42),\n",
       " (0.9075649976730347, 48)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_scores = []\n",
    "for idx, detection in enumerate(detections):\n",
    "    if len(detection['det_bboxes']) > 0:\n",
    "        max_score = max([d.score for d in detection['det_bboxes']])\n",
    "        max_scores.append((max_score, idx))\n",
    "max_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top-n images ranked by their maximum over all detection scores \n",
    "\n",
    "\n",
    "preds = np.argmax(pred_outs, axis=1)\n",
    "wrong_ids = np.where(preds!=data.classes.index('negative'))[0]\n",
    "wrong_ids_confs = [(i, pred_outs[i][preds[i]]) for i in wrong_ids]\n",
    "wrong_ids_confs = sorted(wrong_ids_confs, key=lambda l:l[1], reverse=True)[:NEGATIVE_NUM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sample_ids = [w[0] for w in wrong_ids_confs]\n",
    "negative_sample_labels = [f\"Pred: {data.classes[preds[w[0]]]}\\nConf: {w[1]:.3f}\" for w in wrong_ids_confs]\n",
    "show_ims(neg_data.train_ds.items[negative_sample_ids], negative_sample_labels, rows=NEGATIVE_NUM//5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Add hard negative samples to the training set *T*\n",
    "\n",
    "We add the hard negative samples into the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_files(neg_data.train_ds.items[negative_sample_ids], datapath/'train'/'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the dataset which includes more negative-samples\n",
    "data = (ImageList.from_folder(datapath)\n",
    "        .split_by_folder()\n",
    "        .label_from_folder()\n",
    "        .transform(size=IMAGE_SIZE) \n",
    "        .databunch(bs=BATCH_SIZE, num_workers = db_num_workers()) \n",
    "        .normalize(imagenet_stats))\n",
    "print(data.batch_stats)\n",
    "\n",
    "# Set the dataset to the learner\n",
    "learn.data = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's go **back** to the \"[3. Train the model on T](#train)\" and repeat the training and negative mining steps while we have a decent accuracy on `negative` samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, show the number of repetitions you went through the negative mining\n",
    "print(f\"Ran {len(interpretations)} time(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve some of the notebook outputs\n",
    "sb.glue(\"train_acc\", train_acc)\n",
    "sb.glue(\"valid_acc\", valid_acc)\n",
    "sb.glue(\"negative_sample_ids\", negative_sample_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
